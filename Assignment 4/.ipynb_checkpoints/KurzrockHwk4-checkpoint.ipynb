{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This project is an extension of assignment 2, to a 2D feature space and to\n",
    "####  a classification problem. You will note that the accuracy of the model\n",
    "####  is not that high, suggesting that the model may not be the most\n",
    "####  appropriate for the proposed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, the (simulated) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# red class  =  class 0\n",
    "\n",
    "x0 = np.random.normal(2, 1, (60, 1))\n",
    "y0 = np.random.normal(1, 1, (60, 1))\n",
    "\n",
    "# green class  =  class 1\n",
    "\n",
    "x1 = np.random.normal(5, 1, (70, 1))\n",
    "y1 = np.random.normal(4, 1, (70, 1))\n",
    "\n",
    "# blue class  =  class 2\n",
    "\n",
    "x2 = np.random.normal(3, 1, (80, 1))\n",
    "y2 = np.random.normal(7, 1, (80, 1))\n",
    "\n",
    "# yellow class  =  class 3\n",
    "\n",
    "x3 = np.random.normal(8, 1, (90, 1))\n",
    "y3 = np.random.normal(0, 1, (90, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6b21dee748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x0, y0, c='r', marker='o', s=40)\n",
    "plt.scatter(x1, y1, c='g', marker='x', s=40)\n",
    "plt.scatter(x2, y2, c='b', marker='x', s=40)\n",
    "plt.scatter(x3, y3, c='y', marker='o', s=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Combine the data into a single tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d0 = np.hstack((x0, y0))\n",
    "d1 = np.hstack((x1, y1))\n",
    "d2 = np.hstack((x2, y2))\n",
    "d3 = np.hstack((x3, y3))\n",
    "\n",
    "feature_xy = np.vstack((d0, d1, d2, d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   show shape of feature_xy\n",
    "feature_xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make one-hot encoding of the (desired) target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_class = np.matrix([[1., 0., 0., 0.]] * len(x0) \\\n",
    "                        +  [[0., 1., 0., 0.]] * len(x1) \\\n",
    "                        +  [[0., 0., 1., 0.]] * len(x2) \\\n",
    "                        +  [[0., 0., 0., 1.]] * len(x3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data points (and corresponding classes)\n",
    "### to avoid skewing the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(feature_xy.shape[0])\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "feature_xy = feature_xy[indices, :]\n",
    "\n",
    "expected_class = expected_class[indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters.   Play with these, to see if they make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "training_epochs = 10000\n",
    "\n",
    "num_classes = 4\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The feature tensor shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_size, num_features = feature_xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Make placeholders X and Y for model inputs and corresponding desired classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=tf.placeholder(\"float\", shape=[None,2])\n",
    "Y=tf.placeholder(\"float\", shape=[None,2])\n",
    "#[None,2] need two features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make variables W and b for the weights and biases of the TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W1 is based on training size by num features (reversed)\n",
    "W1=tf.Variable(tf.random_normal([num_features,training_size], stddev=0.03), name='W1')\n",
    "#To get the class matrix probobility\n",
    "#W2 is num features by num classes\n",
    "W2=tf.Variable(tf.random_normal([training_size,num_classes], stddev=0.03), name='W2')\n",
    "\n",
    "#bias 1 is based on training size\n",
    "b1=tf.Variable(tf.random_normal([training_size]), name='b1')\n",
    "#Also used for the number of classes we want\n",
    "#bias 2 based on num of num of classes\n",
    "b2=tf.Variable(tf.random_normal([num_classes], name='b2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Define the model, i.e. the model output.  I just call it model.\n",
    "###      Use the sigmoid activation. Softmax the output, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = tf.sigmoid(tf.add(tf.matmul(X,W1), b1))\n",
    "model = tf.add(tf.matmul(out1,W2),b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the loss (cost) function as the cross-entropy \n",
    "### between desired class and actual class computed by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logit it output of the model. No need for reduce sum with cross entropy function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define minimizer for the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make the Session object, and\n",
    "###     initialize global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.11973\n",
      "200 0.688706\n",
      "400 0.38509\n",
      "600 0.234182\n",
      "800 0.192549\n",
      "1000 0.167613\n",
      "1200 0.130205\n",
      "1400 0.139052\n",
      "1600 0.125227\n",
      "1800 0.103119\n",
      "2000 0.120969\n",
      "2200 0.107759\n",
      "2400 0.0906988\n",
      "2600 0.111575\n",
      "2800 0.0981341\n",
      "3000 0.0834859\n",
      "3200 0.10562\n",
      "3400 0.0919535\n",
      "3600 0.078737\n",
      "3800 0.101402\n",
      "4000 0.0875916\n",
      "4200 0.0753614\n",
      "4400 0.0982032\n",
      "4600 0.0843113\n",
      "4800 0.0728345\n",
      "5000 0.0956651\n",
      "5200 0.0817304\n",
      "5400 0.0708701\n",
      "5600 0.0935859\n",
      "5800 0.0796303\n",
      "6000 0.0692973\n",
      "6200 0.0918416\n",
      "6400 0.0778769\n",
      "6600 0.0680076\n",
      "6800 0.0903514\n",
      "7000 0.0763829\n",
      "7200 0.0669287\n",
      "7400 0.0890593\n",
      "7600 0.0750889\n",
      "7800 0.0660106\n",
      "8000 0.0879254\n",
      "8200 0.0739529\n",
      "8400 0.0652175\n",
      "8600 0.0869202\n",
      "8800 0.0729445\n",
      "9000 0.0645236\n",
      "9200 0.086021\n",
      "9400 0.0720407\n",
      "9600 0.0639096\n",
      "9800 0.0852106\n",
      "10000 0.0712241\n",
      "10200 0.0633609\n",
      "10400 0.0844751\n",
      "10600 0.0704812\n",
      "10800 0.0628663\n",
      "11000 0.0838036\n",
      "11200 0.0698011\n",
      "11400 0.0624171\n",
      "11600 0.0831872\n",
      "11800 0.0691752\n",
      "12000 0.0620064\n",
      "12200 0.0826187\n",
      "12400 0.0685966\n",
      "12600 0.0616288\n",
      "12800 0.0820918\n",
      "13000 0.0680593\n",
      "13200 0.0612795\n",
      "13400 0.0816017\n",
      "13600 0.0675586\n",
      "13800 0.0609552\n",
      "14000 0.081144\n",
      "14200 0.0670905\n",
      "14400 0.0606528\n",
      "14600 0.0807152\n",
      "14800 0.0666515\n",
      "15000 0.0603697\n",
      "15200 0.0803122\n",
      "15400 0.0662386\n",
      "15600 0.060104\n",
      "15800 0.0799325\n",
      "16000 0.0658493\n",
      "16200 0.0598537\n",
      "16400 0.0795736\n",
      "16600 0.0654816\n",
      "16800 0.0596174\n",
      "17000 0.0792335\n",
      "17200 0.0651334\n",
      "17400 0.0593937\n",
      "17600 0.0789107\n",
      "17800 0.064803\n",
      "18000 0.0591816\n",
      "18200 0.0786036\n",
      "18400 0.064489\n",
      "18600 0.0589798\n",
      "18800 0.0783109\n",
      "19000 0.0641901\n",
      "19200 0.0587878\n",
      "19400 0.0780313\n",
      "19600 0.0639051\n",
      "19800 0.0586046\n",
      "20000 0.0777638\n",
      "20200 0.0636329\n",
      "20400 0.0584296\n",
      "20600 0.0775076\n",
      "20800 0.0633727\n",
      "21000 0.0582621\n",
      "21200 0.0772618\n",
      "21400 0.0631236\n",
      "21600 0.0581018\n",
      "21800 0.0770255\n",
      "22000 0.0628848\n",
      "22200 0.0579478\n",
      "22400 0.0767982\n",
      "22600 0.0626556\n",
      "22800 0.0578001\n",
      "23000 0.0765792\n",
      "23200 0.0624354\n",
      "23400 0.057658\n",
      "23600 0.0763681\n",
      "23800 0.0622236\n",
      "24000 0.0575212\n",
      "24200 0.0761642\n",
      "24400 0.0620197\n",
      "24600 0.0573894\n",
      "24800 0.0759672\n",
      "25000 0.0618233\n",
      "25200 0.0572623\n",
      "25400 0.0757766\n",
      "25600 0.0616338\n",
      "25800 0.0571396\n",
      "26000 0.0755921\n",
      "26200 0.0614509\n",
      "26400 0.0570211\n",
      "26600 0.0754132\n",
      "26800 0.0612742\n",
      "27000 0.0569065\n",
      "27200 0.0752397\n",
      "27400 0.0611033\n",
      "27600 0.0567956\n",
      "27800 0.0750714\n",
      "28000 0.0609379\n",
      "28200 0.0566883\n",
      "28400 0.0749079\n",
      "28600 0.0607779\n",
      "28800 0.0565843\n",
      "29000 0.0747488\n",
      "29200 0.0606228\n",
      "29400 0.0564834\n",
      "29600 0.0745942\n",
      "29800 0.0604723\n"
     ]
    }
   ],
   "source": [
    "for step in range(training_epochs * training_size // batch_size):\n",
    "    offset = (step * batch_size) % training_size\n",
    "    batch_data = feature_xy[offset:(offset + batch_size), :]\n",
    "    batch_classes = expected_class[offset:(offset + batch_size)]\n",
    "    err, _ = sess.run([loss, minimizer], feed_dict={X: batch_data, Y: batch_classes})\n",
    "    if step % 200 == 0:\n",
    "        print (step, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  7. Show the weights and biases obtained for the model, after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned W [[-0.3866176   0.07681652 -0.00808992 -0.0592291  -0.02181638 -0.03471012\n",
      "  -0.25721997 -0.22485094  0.06565104 -0.3020747   0.01896844  0.28094274\n",
      "   0.17867696 -0.21727619 -0.09448799  0.09514847  0.16039245 -0.40237635\n",
      "  -0.14099206 -0.44220906  0.32965234 -0.24321747 -0.11795603  0.2927503\n",
      "   0.39647979 -0.27573118 -0.38478532 -0.2670922  -0.09077352 -0.22174709\n",
      "  -0.05323688 -0.33596775  0.35407197 -0.28074974 -0.34080124 -0.30785775\n",
      "   0.10637208 -0.1923856  -0.39065436  0.15545765  0.15693666 -0.02699632\n",
      "  -0.0695513  -0.32420614 -0.17450829  0.17846628 -0.42229411 -0.4225271\n",
      "  -0.212107    0.34071285 -0.29434848 -0.04060499 -0.2486286  -0.20806871\n",
      "   0.37402099 -0.07013786 -0.3248271   0.36574736 -0.2375368  -0.30400959\n",
      "  -0.43336096 -0.5213539  -0.01897461 -0.09868034 -0.38710082 -0.11140012\n",
      "  -0.18306883 -0.08143695  0.25527883 -0.15424256 -0.174999   -0.17920433\n",
      "   0.1312104   0.24402225 -0.15122291  0.2187123  -0.42971596  0.19217104\n",
      "   0.17001012  0.28235662 -0.28514025  0.08574374 -0.40200001 -0.27068734\n",
      "  -0.13937011 -0.23150119 -0.04828421  0.37617147  0.04384857 -0.27329767\n",
      "  -0.18161067 -0.24817204 -0.02311261  0.33266136 -0.27331206 -0.17644916\n",
      "   0.04034963  0.1011987   0.32286015 -0.39195335 -0.11075467  0.05055175\n",
      "  -0.35168856 -0.18241452  0.41731796  0.37936392  0.30152762  0.11388978\n",
      "  -0.07740485  0.00163946  0.12899689  0.18783182 -0.1750405   0.06303409\n",
      "   0.29506561  0.3587555  -0.2599794  -0.12972823 -0.10013221  0.48702815\n",
      "  -0.20423833 -0.28008029  0.16134201  0.30765674 -0.34422556  0.33143193\n",
      "  -0.30886856  0.37352067  0.33960429 -0.14730567 -0.36272877  0.33647272\n",
      "   0.0836242  -0.33036873 -0.42586705 -0.0304798   0.06855176  0.27393147\n",
      "  -0.27187189  0.1829209  -0.39000872 -0.05812924 -0.02119813 -0.2309875\n",
      "   0.01835382 -0.27808756  0.30717748  0.30960095 -0.21882658 -0.38936594\n",
      "  -0.20757838 -0.27560535 -0.1746358  -0.23897813 -0.34249884  0.34726539\n",
      "  -0.16995881 -0.23965792 -0.00680604 -0.32376987 -0.19044016  0.16158928\n",
      "   0.13826197 -0.20144372  0.24795009  0.0013924   0.13223545  0.37335643\n",
      "   0.19115159 -0.29042923 -0.21653488 -0.38858557 -0.10100653  0.1536338\n",
      "  -0.11519744 -0.32452941  0.2681987  -0.02531097 -0.01754033 -0.34242812\n",
      "   0.13202125 -0.00754677 -0.31656343  0.3294135   0.2042962  -0.26583296\n",
      "  -0.12175537 -0.06414843  0.4449859  -0.26381722 -0.11380635  0.36399341\n",
      "   0.18939936 -0.17435606 -0.17209046 -0.24270609  0.24853766 -0.04822936\n",
      "  -0.22999999  0.23820047 -0.2398866   0.03393024 -0.12910421 -0.17813472\n",
      "   0.21931808  0.11844257 -0.31407166 -0.39056307 -0.14624698  0.30169955\n",
      "  -0.14887559  0.10769606 -0.33301002  0.20275772  0.06862105 -0.06206235\n",
      "  -0.36968714  0.12828986 -0.2488496  -0.07123729 -0.32233572  0.17594948\n",
      "   0.35910803 -0.27557689 -0.18651445 -0.31212044  0.18149543 -0.28322178\n",
      "   0.26588079  0.02037629 -0.25917426 -0.43241483 -0.2332983  -0.26711747\n",
      "  -0.41361922 -0.10775537 -0.34512067 -0.21223266 -0.28670707 -0.14866956\n",
      "  -0.40056562 -0.4138034   0.2463709  -0.16677192 -0.18271428  0.02735443\n",
      "  -0.2738449  -0.29450783  0.07924179  0.03916239 -0.12213835  0.21077715\n",
      "  -0.00790237  0.25643182  0.40926528 -0.36091799 -0.12831041 -0.10053591\n",
      "   0.22699779 -0.17046878  0.51903766 -0.49997771 -0.27092275 -0.10929433\n",
      "  -0.18822852  0.11881741  0.12383838 -0.25285146  0.27061993  0.3948642\n",
      "  -0.37888479 -0.17427967  0.30238622  0.03646079 -0.14865404  0.31520468\n",
      "   0.05023937 -0.0633081  -0.18627964 -0.27743727 -0.0925571  -0.0430351\n",
      "   0.18809961  0.1975524  -0.18154262 -0.2527611  -0.42276698  0.18705097\n",
      "   0.12573309  0.09218361  0.11977668 -0.0265932   0.48190162  0.40582347\n",
      "  -0.05748022 -0.27245608 -0.01679179  0.25579837  0.36217687 -0.26681644]\n",
      " [-0.00183871 -0.18530484 -0.14806817  0.14109553  0.18237595 -0.18434305\n",
      "   0.30120701  0.22655512 -0.13102192 -0.14956437 -0.3687371   0.28172165\n",
      "   0.20965636  0.4412964  -0.00341446 -0.25425076 -0.0492087  -0.10060769\n",
      "  -0.07555613 -0.08820784 -0.26114678  0.04963486 -0.12626414 -0.31585872\n",
      "  -0.32537082  0.37704122 -0.22306356  0.31546825 -0.03075841 -0.32136777\n",
      "  -0.14411108  0.21528134  0.29445064  0.01507988 -0.10365602 -0.23014662\n",
      "  -0.14276774 -0.25883496  0.15587765  0.34924904 -0.12899682 -0.00134845\n",
      "   0.45823121  0.41749439 -0.50341433 -0.04219623 -0.14623554  0.37925994\n",
      "  -0.20107338 -0.3691721  -0.1959839   0.05782665  0.39571521 -0.1134078\n",
      "  -0.27275756 -0.19672135  0.03695351 -0.16019778 -0.23149198 -0.08271272\n",
      "  -0.00520259  0.12529416  0.06572797  0.08010239  0.13454308  0.6379022\n",
      "   0.2459532   0.02726661 -0.62926441 -0.35529426  0.2244022  -0.57430971\n",
      "   0.46502635 -0.42454621 -0.09289367  0.17952892 -0.21765389 -0.57818258\n",
      "  -0.20246905  0.17702033  0.39023092 -0.23869456 -0.02722847 -0.25582525\n",
      "   0.34584185 -0.20503886 -0.23689292  0.03309908 -0.03006015 -0.22793131\n",
      "  -0.14419623 -0.05701559 -0.03451718 -0.40293419  0.18531108  0.39758745\n",
      "   0.08725592  0.29260853 -0.14622822 -0.02733556 -0.01573842  0.26485214\n",
      "   0.2412582  -0.30646113 -0.05977917 -0.20869742 -0.54478198 -0.2459341\n",
      "   0.05850507 -0.46667817  0.26935104  0.10984638 -0.18492836 -0.12156133\n",
      "  -0.50944412 -0.23706573 -0.01253347 -0.28789991 -0.38286763 -0.28799295\n",
      "   0.07635038  0.40735444 -0.17168462 -0.34762937  0.04059184 -0.11038031\n",
      "  -0.22896603 -0.18468916 -0.1854734   0.40434793 -0.27083513 -0.33380589\n",
      "  -0.158309   -0.40195358  0.11707273  0.23847404 -0.12578872 -0.02109747\n",
      "  -0.17941183  0.10119621  0.27511203 -0.09738745 -0.15356159 -0.38763955\n",
      "   0.00584017 -0.23940454 -0.35038131 -0.23271732 -0.04250348  0.26166928\n",
      "   0.07339323 -0.24039593 -0.1936896   0.35074666  0.37742674 -0.08089491\n",
      "   0.04336759  0.34135899 -0.02475995  0.24018253 -0.35009304 -0.43800995\n",
      "  -0.47233084 -0.49890089 -0.22248819 -0.4357397  -0.37744248  0.18210962\n",
      "   0.39971873  0.03464809 -0.3580943   0.169406    0.02508088 -0.32143024\n",
      "  -0.21006335 -0.08322975 -0.61541432 -0.16409361  0.36041862  0.15903775\n",
      "   0.29101118  0.03439851  0.1518479  -0.14205304 -0.32996297  0.07141668\n",
      "  -0.37625241 -0.47603753 -0.55758399  0.36709374 -0.28021038  0.06156395\n",
      "  -0.60625839 -0.31295326  0.06383361 -0.40296462  0.07145415 -0.48260814\n",
      "  -0.00073074 -0.35282981  0.53089976  0.05336109  0.59823954  0.27601898\n",
      "   0.26532099 -0.00938899 -0.14282949 -0.06615523  0.58719122  0.18491544\n",
      "   0.44127697 -0.10524274  0.27006054 -0.47076458  0.23498623 -0.15921299\n",
      "   0.27036807  0.11342099  0.68229848  0.455486    0.29106477 -0.34736046\n",
      "  -0.51506555 -0.23173171 -0.40715486  0.30467534 -0.37465936 -0.13608621\n",
      "   0.22755413  0.4769842   0.02170469  0.22761041  0.3871358   0.52893531\n",
      "   0.14335896  0.13245852 -0.01890345 -0.06144443 -0.30610377 -0.16142745\n",
      "   0.44964984 -0.01210876 -0.36139199 -0.2201554   0.08429859 -0.1958728\n",
      "   0.18683429  0.38571665  0.04826703  0.44432014  0.50420308  0.23648173\n",
      "   0.06129588 -0.42795065 -0.35547289  0.34903559  0.28552151  0.38177234\n",
      "  -0.31452909 -0.26268819 -0.14170551  0.02937934  0.22466126 -0.09602513\n",
      "  -0.11002956  0.5050351  -0.30420235  0.01556653 -0.33431339 -0.22721991\n",
      "  -0.03814659  0.3509436   0.15271647  0.3073892  -0.06209338  0.22061275\n",
      "   0.42122796 -0.3577528  -0.26652977 -0.20630379  0.38529143 -0.04952213\n",
      "  -0.12849456 -0.02956209  0.14759809  0.44379303  0.26298514  0.16370599\n",
      "  -0.52460068 -0.30914906  0.01382222 -0.17471205 -0.32284111 -0.33279067\n",
      "  -0.47730047  0.24571896  0.05080555  0.18649743  0.19549555  0.41713136]]\n",
      "Learned b [ 1.70246351 -0.39677367  1.66260386 -1.88132668  0.45531821 -0.50988102\n",
      " -0.263125   -0.5385856   1.99352574  0.44935563  0.02169677 -1.08547175\n",
      "  0.2598004  -1.063802    0.91116357 -0.94812113 -0.91654408  1.42339623\n",
      " -0.74425477  1.57473457 -0.65127426 -0.57595915  2.43737078 -0.33930916\n",
      " -0.04397742  0.13828777  1.39623427 -0.96433914 -1.00767243  1.14573061\n",
      " -1.46489429  0.70210582 -1.14712071 -0.37097868  0.73682481  0.74375689\n",
      " -1.13264358  0.48293921  0.6800496  -1.01877332  0.09267019 -2.11966467\n",
      " -0.48678067  0.2317073   1.37027013 -1.87954092  1.45356059  1.09163773\n",
      " -0.07328843 -0.45310447  0.99209118 -1.7408272  -0.30662814 -0.14449771\n",
      " -0.62557322 -0.74977714  0.76410919 -0.71574038  0.45203412  1.66743934\n",
      "  1.00424266  1.3780781   1.92312717 -1.01129878  0.82799673 -1.33598661\n",
      "  0.28039509  1.73022842  0.19605204  0.51265144 -0.92724597  1.40172243\n",
      " -0.88913751  0.363013   -0.57101673 -0.08222077  1.80758059  0.39878806\n",
      " -0.8201015  -0.61175555 -0.36810049 -1.00762224  1.10701716  0.55018866\n",
      " -0.69091016 -0.00540272 -0.09922256 -1.23289573 -1.22940743  0.67646825\n",
      " -0.43198088  2.29326367 -2.40436745  0.40782854  0.67930895 -0.78653282\n",
      " -0.84281087 -1.93130374 -0.10674902  1.05498838 -2.0932169   0.60374796\n",
      "  0.98873544  0.30444232 -1.17686176 -1.1902864  -0.20533746 -0.70037943\n",
      " -1.55297637  1.10115099  0.37768081  0.49652809 -0.47557765 -0.55797565\n",
      " -0.19629189  0.03563939 -0.20413278  0.44127807  0.73693579 -1.73081887\n",
      " -0.74342865 -0.00756926 -0.40278205 -0.66799599  1.69830918 -1.1840353\n",
      "  0.70188999 -0.44605476 -0.57481527 -0.85940951  1.53554034 -0.8901065\n",
      " -0.98885381  1.72576201  1.24698925  0.45011565  1.4536823   0.03300714\n",
      "  0.46868059  0.36932248 -0.10683387 -0.89885169  1.19386685  0.97805709\n",
      "  3.16938043  1.44079864 -0.2136364   0.22348341 -0.71416348  0.75673699\n",
      "  1.26583934  0.6507495  -0.26703545  0.07338072 -0.03794009 -0.0766039\n",
      " -1.46914148 -0.31110784  2.60005307  0.17116867  1.46929252  0.41379052\n",
      "  0.06406567  1.26089704 -0.10702686  0.47920313  0.34712321 -1.10541654\n",
      " -0.66257906  0.17852682  1.43936086  0.1644281  -0.27234861  0.14749987\n",
      " -0.42668039  0.19908129  0.20249957  2.5424571  -1.76532543  0.14709406\n",
      " -0.23059489 -2.53693438  0.62089086 -1.20693898  0.38797083 -0.0547078\n",
      "  0.40742499  1.09386194 -1.11360717  0.14113115  0.78764468 -0.85768217\n",
      "  0.48183423  0.21169701  0.14274438  1.60057831 -1.4143039   1.62006974\n",
      " -0.74601775  0.02004326 -0.31942818 -1.63043189 -0.73272157 -0.97555423\n",
      " -0.32051137  1.36689377  0.63252008  1.05907452 -0.83305722 -0.48768821\n",
      " -0.97941577  0.18733217 -0.01619657  0.55196208  0.45373881 -0.53694546\n",
      "  0.65308022  0.66349697 -0.32990673 -0.70207554 -0.62028855 -0.28944215\n",
      " -0.59487003  0.82768941  0.66743189  0.35735992  0.29206479  0.11862846\n",
      " -1.11049485 -1.18609405  0.49265844  0.67269534 -0.61914372 -0.05241449\n",
      "  0.67787856  1.43061101  0.24842709 -0.11721624  1.74777794  1.3243196\n",
      "  0.87756276  0.95421076 -0.11583585 -0.23076613 -0.90517867  0.3981263\n",
      " -0.17527299  0.16813287 -2.28392887 -1.13165069 -0.59749782 -0.31050557\n",
      " -1.94958091 -0.11676919 -0.28160813 -0.51914251  0.07531894 -0.89666432\n",
      "  0.5109365   0.73193008 -2.06536222  1.7125634   0.33391744 -1.11005592\n",
      "  0.19063105 -0.82226896  0.16685162 -0.38163662 -0.2239219  -0.84010446\n",
      "  0.87125754  0.61311197 -0.52646995 -2.13536954  2.70086527 -0.87255943\n",
      " -0.62965238  1.06972265  0.24293292  0.69537622  0.00892934 -2.17298436\n",
      " -1.34181213 -1.47193849  0.03502809 -0.05583416  0.42752129  0.00400347\n",
      "  0.72276217  0.21202688  0.67588073 -0.08162338 -0.16990528 -0.20581885\n",
      "  1.04784703 -0.46375781 -0.47100985 -0.26295289 -1.26844454  0.21861608]\n"
     ]
    }
   ],
   "source": [
    "learned_w1=sess.run(W1)\n",
    "learned_b1=sess.run(b1)\n",
    "print(\"Learned W\", learned_w1)\n",
    "print(\"Learned b\", learned_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Make a few predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "feed_dict = { X: [[8.0,-1.0]] }    # input X must be of shape [None,2]\n",
    "class_vector = sess.run(model, feed_dict)\n",
    "class_number = sess.run(tf.argmax(class_vector,1))\n",
    "print(class_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Ian's Predicitions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#I believe this output for this will be class 0\n",
    "feed_dict = { X: [[2.0,2.0]] }    # input X must be of shape [None,2]\n",
    "class_vector = sess.run(model, feed_dict)\n",
    "class_number = sess.run(tf.argmax(class_vector,1))\n",
    "print(class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#I believe this output for this will be class 1\n",
    "feed_dict = { X: [[4.5,4.5]] }    # input X must be of shape [None,2]\n",
    "class_vector = sess.run(model, feed_dict)\n",
    "class_number = sess.run(tf.argmax(class_vector,1))\n",
    "print(class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "#I believe this output for this will be class 2\n",
    "feed_dict = { X: [[8.0,2.5]] }    # input X must be of shape [None,2]\n",
    "class_vector = sess.run(model, feed_dict)\n",
    "class_number = sess.run(tf.argmax(class_vector,1))\n",
    "print(class_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an accuracy node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_accuracy = sess.run([accuracy], feed_dict={X: feature_xy, Y: expected_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98333335]\n"
     ]
    }
   ],
   "source": [
    "print(training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 8. Make test data similar to the provided data above.\n",
    "###     Just 10 datapoints for each of the 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Notice the 10 in the paranthesis, makes 10 more points as specified\n",
    "\n",
    "x0 = np.random.normal(3, 1, (10, 1))\n",
    "y0 = np.random.normal(2, 1, (10, 1))\n",
    "\n",
    "\n",
    "x1 = np.random.normal(4, 1, (10, 1))\n",
    "y1 = np.random.normal(3, 1, (10, 1))\n",
    "\n",
    "\n",
    "x2 = np.random.normal(4, 1, (10, 1))\n",
    "y2 = np.random.normal(6, 1, (10, 1))\n",
    "\n",
    "\n",
    "x3 = np.random.normal(9, 1, (10, 1))\n",
    "y3 = np.random.normal(1, 1, (10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  9. Run the training on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d0 = np.hstack((x0, y0))\n",
    "d1 = np.hstack((x1, y1))\n",
    "d2 = np.hstack((x2, y2))\n",
    "d3 = np.hstack((x3, y3))\n",
    "\n",
    "feature_xy = np.vstack((d0, d1, d2, d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   show shape of feature_xy\n",
    "feature_xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_class = np.matrix([[1., 0., 0., 0.]] * len(x0) \\\n",
    "                        +  [[0., 1., 0., 0.]] * len(x1) \\\n",
    "                        +  [[0., 0., 1., 0.]] * len(x2) \\\n",
    "                        +  [[0., 0., 0., 1.]] * len(x3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(feature_xy.shape[0])\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "feature_xy = feature_xy[indices, :]\n",
    "\n",
    "expected_class = expected_class[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_size, num_features = feature_xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#W1 is based on training size by num features (reversed)\n",
    "W1=tf.Variable(tf.random_normal([num_features,training_size], stddev=0.03), name='W1')\n",
    "#To get the class matrix probobility\n",
    "#W2 is num features by num classes\n",
    "W2=tf.Variable(tf.random_normal([training_size,num_classes], stddev=0.03), name='W2')\n",
    "\n",
    "#bias 1 is based on training size\n",
    "b1=tf.Variable(tf.random_normal([training_size]), name='b1')\n",
    "#Also used for the number of classes we want\n",
    "#bias 2 based on num of num of classes\n",
    "b2=tf.Variable(tf.random_normal([num_classes], name='b2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out1 = tf.sigmoid(tf.add(tf.matmul(X,W1), b1))\n",
    "model = tf.add(tf.matmul(out1,W2),b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logit it output of the model. No need for reduce sum with cross entropy function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.11973\n",
      "200 0.688706\n",
      "400 0.38509\n",
      "600 0.234182\n",
      "800 0.192549\n",
      "1000 0.167613\n",
      "1200 0.130205\n",
      "1400 0.139052\n",
      "1600 0.125227\n",
      "1800 0.103119\n",
      "2000 0.120969\n",
      "2200 0.107759\n",
      "2400 0.0906988\n",
      "2600 0.111575\n",
      "2800 0.0981341\n",
      "3000 0.0834859\n",
      "3200 0.10562\n",
      "3400 0.0919535\n",
      "3600 0.078737\n",
      "3800 0.101402\n",
      "4000 0.0875916\n",
      "4200 0.0753614\n",
      "4400 0.0982032\n",
      "4600 0.0843113\n",
      "4800 0.0728345\n",
      "5000 0.0956651\n",
      "5200 0.0817304\n",
      "5400 0.0708701\n",
      "5600 0.0935859\n",
      "5800 0.0796303\n",
      "6000 0.0692973\n",
      "6200 0.0918416\n",
      "6400 0.0778769\n",
      "6600 0.0680076\n",
      "6800 0.0903514\n",
      "7000 0.0763829\n",
      "7200 0.0669287\n",
      "7400 0.0890593\n",
      "7600 0.0750889\n",
      "7800 0.0660106\n",
      "8000 0.0879254\n",
      "8200 0.0739529\n",
      "8400 0.0652175\n",
      "8600 0.0869202\n",
      "8800 0.0729445\n",
      "9000 0.0645236\n",
      "9200 0.086021\n",
      "9400 0.0720407\n",
      "9600 0.0639096\n",
      "9800 0.0852106\n",
      "10000 0.0712241\n",
      "10200 0.0633609\n",
      "10400 0.0844751\n",
      "10600 0.0704812\n",
      "10800 0.0628663\n",
      "11000 0.0838036\n",
      "11200 0.0698011\n",
      "11400 0.0624171\n",
      "11600 0.0831872\n",
      "11800 0.0691752\n",
      "12000 0.0620064\n",
      "12200 0.0826187\n",
      "12400 0.0685966\n",
      "12600 0.0616288\n",
      "12800 0.0820918\n",
      "13000 0.0680593\n",
      "13200 0.0612795\n",
      "13400 0.0816017\n",
      "13600 0.0675586\n",
      "13800 0.0609552\n",
      "14000 0.081144\n",
      "14200 0.0670905\n",
      "14400 0.0606528\n",
      "14600 0.0807152\n",
      "14800 0.0666515\n",
      "15000 0.0603697\n",
      "15200 0.0803122\n",
      "15400 0.0662386\n",
      "15600 0.060104\n",
      "15800 0.0799325\n",
      "16000 0.0658493\n",
      "16200 0.0598537\n",
      "16400 0.0795736\n",
      "16600 0.0654816\n",
      "16800 0.0596174\n",
      "17000 0.0792335\n",
      "17200 0.0651334\n",
      "17400 0.0593937\n",
      "17600 0.0789107\n",
      "17800 0.064803\n",
      "18000 0.0591816\n",
      "18200 0.0786036\n",
      "18400 0.064489\n",
      "18600 0.0589798\n",
      "18800 0.0783109\n",
      "19000 0.0641901\n",
      "19200 0.0587878\n",
      "19400 0.0780313\n",
      "19600 0.0639051\n",
      "19800 0.0586046\n",
      "20000 0.0777638\n",
      "20200 0.0636329\n",
      "20400 0.0584296\n",
      "20600 0.0775076\n",
      "20800 0.0633727\n",
      "21000 0.0582621\n",
      "21200 0.0772618\n",
      "21400 0.0631236\n",
      "21600 0.0581018\n",
      "21800 0.0770255\n",
      "22000 0.0628848\n",
      "22200 0.0579478\n",
      "22400 0.0767982\n",
      "22600 0.0626556\n",
      "22800 0.0578001\n",
      "23000 0.0765792\n",
      "23200 0.0624354\n",
      "23400 0.057658\n",
      "23600 0.0763681\n",
      "23800 0.0622236\n",
      "24000 0.0575212\n",
      "24200 0.0761642\n",
      "24400 0.0620197\n",
      "24600 0.0573894\n",
      "24800 0.0759672\n",
      "25000 0.0618233\n",
      "25200 0.0572623\n",
      "25400 0.0757766\n",
      "25600 0.0616338\n",
      "25800 0.0571396\n",
      "26000 0.0755921\n",
      "26200 0.0614509\n",
      "26400 0.0570211\n",
      "26600 0.0754132\n",
      "26800 0.0612742\n",
      "27000 0.0569065\n",
      "27200 0.0752397\n",
      "27400 0.0611033\n",
      "27600 0.0567956\n",
      "27800 0.0750714\n",
      "28000 0.0609379\n",
      "28200 0.0566883\n",
      "28400 0.0749079\n",
      "28600 0.0607779\n",
      "28800 0.0565843\n",
      "29000 0.0747488\n",
      "29200 0.0606228\n",
      "29400 0.0564834\n",
      "29600 0.0745942\n",
      "29800 0.0604723\n"
     ]
    }
   ],
   "source": [
    "for step in range(training_epochs * training_size // batch_size):\n",
    "    offset = (step * batch_size) % training_size\n",
    "    batch_data = feature_xy[offset:(offset + batch_size), :]\n",
    "    batch_classes = expected_class[offset:(offset + batch_size)]\n",
    "    err, _ = sess.run([loss, minimizer], feed_dict={X: batch_data, Y: batch_classes})\n",
    "    if step % 200 == 0:\n",
    "        print (step, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  10. Use accuracy node on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
